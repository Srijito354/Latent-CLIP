{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the datset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "with open('data/easy-VQA/easy_vqa/data/train/questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "image_paths = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in data:\n",
    "    img_path = f\"data/easy-VQA/easy_vqa/data/train/images/{item[2]}.png\"\n",
    "    question = item[0]\n",
    "    answer = item[1]\n",
    "\n",
    "    image_paths.append(img_path)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyVQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, questions, answers, transform, text_encoder):\n",
    "        self.image_paths = image_paths\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.transform = transform\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        question = self.questions[idx]\n",
    "        input_em = embedding_gen(question)\n",
    "        pe = positional_encodings(vocab_size, embedding_dim)\n",
    "        question_embedding = input_em + pe\n",
    "\n",
    "        answer = self.answers[idx]\n",
    "        input_ans = embedding_gen(answer)\n",
    "        pe = positional_encodings(vocab_size, embedding_dim)\n",
    "        answer_embedding = input_ans + pe\n",
    "\n",
    "        return image, question_embedding, answer_embedding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, questions, answers = zip(*batch)\n",
    "\n",
    "    # Pad questions to same length\n",
    "    padded_images = pad_sequence(images, batch_first=True)\n",
    "    padded_questions = pad_sequence(questions, batch_first=True)\n",
    "    padded_answers = pad_sequence(answers, batch_first=True)\n",
    "\n",
    "    return padded_images, padded_questions, padded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dataloader\n",
    "batch_size = 1\n",
    "train_dataset = EasyVQADataset(image_paths, questions, answers, transform, text_encoder=None)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the image encoder:\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = 16\n",
    "\n",
    "        # Downscaling layers for Q, K, V\n",
    "        self.w_q = nn.Linear(16, 2)\n",
    "        self.w_k = nn.Linear(16, 2)\n",
    "        self.w_v = nn.Linear(16, 2)\n",
    "\n",
    "        # Upscaling back to embedding dim\n",
    "        self.latent_upscale = nn.Linear(2, 16)\n",
    "\n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "\n",
    "        # Feedforward block\n",
    "        self.feed_fwd = nn.Sequential(\n",
    "            nn.Linear(16, 16),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "\n",
    "        # Final projection\n",
    "        self.output_proj = nn.Linear(16, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.w_q(x)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_v(x)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embedding_dim ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        context = self.latent_upscale(context)\n",
    "\n",
    "        # Residual + Norm\n",
    "        x = self.layer_norm(context + x)\n",
    "\n",
    "        # Feedforward + Norm\n",
    "        ff_out = self.feed_fwd(x)\n",
    "        out = self.layer_norm(ff_out + x)\n",
    "\n",
    "        # Final linear (optional)\n",
    "        return self.output_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the text and positional embedding generators\n",
    "\n",
    "def embedding_gen(sentence):\n",
    "    global words\n",
    "    words = sentence.lower().split()\n",
    "    global word_idx\n",
    "    word_idx = {word: idx for idx, word in enumerate(words)}\n",
    "    global embedding_dim\n",
    "    embedding_dim = 16\n",
    "    global vocab_size\n",
    "    global idx_only\n",
    "    idx_only = [i for i in range(len(word_idx))]\n",
    "    vocab_size = len(word_idx)\n",
    "    embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "    input_tensor = torch.LongTensor(idx_only)\n",
    "    input_embeddings = embeddings(input_tensor)\n",
    "    return input_embeddings\n",
    "\n",
    "def positional_encodings(sequence_length, embedding_size):\n",
    "    pe = torch.zeros(sequence_length, embedding_size)\n",
    "    pos_encode = 0\n",
    "    for pos in range(len(word_idx)):\n",
    "        em_dim = embedding_dim\n",
    "        for i in range(em_dim):\n",
    "            if i%2 == 0:\n",
    "                emma = torch.sin(torch.tensor(pos/(10000**((2*i)/embedding_dim))))\n",
    "            else:\n",
    "                emma = torch.cos(torch.tensor(pos/(10000**(((2*i)+1)/embedding_dim))))\n",
    "            pe[pos][i] = emma\n",
    "            #pe[pos][i+1] = emma2\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the text encoder:\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = 16\n",
    "\n",
    "        self.w_q = nn.Linear(16, 2)\n",
    "        self.w_k = nn.Linear(16, 2)\n",
    "        self.w_v = nn.Linear(16, 2)\n",
    "\n",
    "        self.latent_upscale = nn.Linear(2, 16)\n",
    "        self.layer_norm = nn.LayerNorm(16)\n",
    "\n",
    "        self.feed_fwd = nn.Sequential(\n",
    "            nn.Linear(16, 16),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(16, 16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.w_q(x)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_k(x)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embedding_dim ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "\n",
    "        context = self.latent_upscale(context)\n",
    "\n",
    "        # Residual + Norm\n",
    "        x = self.layer_norm(context + x)\n",
    "\n",
    "        # Feedforward + Norm\n",
    "        ff_out = self.feed_fwd(x)\n",
    "        out = self.layer_norm(ff_out + x)\n",
    "\n",
    "        # Final linear (optional)\n",
    "        return self.output_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model:\n",
    "class CLIPMini(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPMini, self).__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "\n",
    "    def forward(self, image_patches, text_tokens):\n",
    "\n",
    "        # Let's assume: image_patches: [B, 196, 16]\n",
    "        cls_token_img = nn.Parameter(torch.randn(1, 1, 16)).to(image_patches)\n",
    "        cls_token_img = cls_token_img.expand(image_patches.size(0), -1, -1)  # [B, 1, 16]\n",
    "\n",
    "        img_input = torch.cat([cls_token_img, image_patches], dim=1)  # [B, 197, 16]\n",
    "        img_embs = self.image_encoder(img_input)  # Encoder will attend to [CLS]\n",
    "\n",
    "        cls_token_txt = nn.Parameter(torch.randn(1, 1, 16)).to(text_tokens.device)\n",
    "        cls_token_txt = cls_token_txt.expand(text_tokens.size(0), -1, -1)  # [B, 1, 16]\n",
    "\n",
    "        txt_input = torch.cat([cls_token_txt, text_tokens], dim=1)  # [B, seq_len+1, 16]\n",
    "        txt_embs = self.text_encoder(txt_input)\n",
    "\n",
    "        '''\n",
    "        img_embs = self.image_encoder(image_patches)  # [B, 196, 16]\n",
    "        txt_embs = self.text_encoder(text_tokens) # [B, seq_len, 16]\n",
    "        '''\n",
    "\n",
    "        # Pool\n",
    "        #img_vec = torch.mean(img_embs, dim=1)      # [B, 16]\n",
    "        img_vec = img_embs[:, 0, :]                # [B, 16] with CLS\n",
    "        txt_vec = txt_embs[:, 0, :]                # [B, 16] with CLS\n",
    "\n",
    "        # Normalize\n",
    "        img_vec = F.normalize(img_vec, dim=-1)\n",
    "        txt_vec = F.normalize(txt_vec, dim=-1)\n",
    "\n",
    "        return img_vec, txt_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPMini()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "pos_mat = nn.Parameter(torch.randn(batch_size, 196, 16))\n",
    "layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(16, 16), stride=16)\n",
    "layer = layer.to(device)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] — Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [2/100] — Loss: 0.0000, Accuracy: 100.00%\n",
      "Epoch [3/100] — Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#img_pass = img_em + pos_mat[:32]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 2. Text encoding\u001b[39;00m\n\u001b[1;32m     23\u001b[0m image_vec, question_vec \u001b[38;5;241m=\u001b[39m model(img_pass, qs_em)\n\u001b[0;32m---> 24\u001b[0m _, ans_vec \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mans_em\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 3. Similarity + logits\u001b[39;00m\n\u001b[1;32m     27\u001b[0m add_vec \u001b[38;5;241m=\u001b[39m image_vec \u001b[38;5;241m+\u001b[39m question_vec               \u001b[38;5;66;03m# [B, D]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m, in \u001b[0;36mCLIPMini.forward\u001b[0;34m(self, image_patches, text_tokens)\u001b[0m\n\u001b[1;32m     18\u001b[0m cls_token_txt \u001b[38;5;241m=\u001b[39m cls_token_txt\u001b[38;5;241m.\u001b[39mexpand(text_tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, 1, 16]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m txt_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cls_token_txt, text_tokens], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, seq_len+1, 16]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m txt_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mimg_embs = self.image_encoder(image_patches)  # [B, 196, 16]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mtxt_embs = self.text_encoder(text_tokens) # [B, seq_len, 16]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Pool\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#img_vec = torch.mean(img_embs, dim=1)      # [B, 16]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[31], line 25\u001b[0m, in \u001b[0;36mTextEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     24\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_q(x)\n\u001b[0;32m---> 25\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_k(x)\n\u001b[1;32m     28\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q, K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for imgs, qs_em, ans_em in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        qs_em = qs_em.to(device)\n",
    "        ans_em = ans_em.to(device)\n",
    "        pos_mat = pos_mat.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Image encoding\n",
    "        img_em = layer(imgs).flatten(2).transpose(1, 2)  # [B, 196, 16]\n",
    "        img_pass = img_em + pos_mat[:imgs.size(0)]       # Ensure correct batch slice\n",
    "        #img_pass = img_em + pos_mat[:32]\n",
    "\n",
    "        # 2. Text encoding\n",
    "        image_vec, question_vec = model(img_pass, qs_em)\n",
    "        _, ans_vec = model(img_pass, ans_em)\n",
    "\n",
    "        # 3. Similarity + logits\n",
    "        add_vec = image_vec + question_vec               # [B, D]\n",
    "        logits = torch.matmul(add_vec, ans_vec.T)        # [B, B]\n",
    "\n",
    "        # 4. Labels = diagonal elements (correct matches)\n",
    "        labels = torch.arange(logits.size(0)).to(device)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5. Stats tracking\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] — Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as clip_mini.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), \"clip_mini2.pth\")\n",
    "print(\"Model saved as clip_mini.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('./data/easy-VQA/easy_vqa/data/test/images/1.png')\n",
    "img = transform(img).unsqueeze(0)\n",
    "layer = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 16, stride = 16)\n",
    "img = layer(img).flatten(2).transpose(1, 2)  # [B, 196, 16]\n",
    "pos_mat = nn.Parameter(torch.randn(1, 196, 16))\n",
    "x = img + pos_mat\n",
    "x = x.to(device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"./data/easy-VQA/easy_vqa/data/answers.txt\")\n",
    "answer_ems = []\n",
    "answer_ems2 = []\n",
    "for word in f.readlines():\n",
    "    answer_ems2.append(word)\n",
    "    emb = embedding_gen(word)\n",
    "    pe = positional_encodings(vocab_size, embedding_dim)\n",
    "    encoded = (emb + pe).to(device)\n",
    "    encoded = encoded.unsqueeze(0).to(device)\n",
    "    _, a_vec = model(x, encoded)\n",
    "    answer_ems.append(a_vec)\n",
    "\n",
    "answer_vec = torch.cat(answer_ems, dim=0)\n",
    "\n",
    "len(answer_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Enter question:\")\n",
    "question_embeddings = embedding_gen(question)\n",
    "pe = positional_encodings(vocab_size, embedding_dim)\n",
    "y = question_embeddings + pe\n",
    "y = y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vec, ques_vec = model(x, y)\n",
    "combined_vec = img_vec + ques_vec  # [B, dim]\n",
    "logits = torch.matmul(combined_vec, answer_vec.T)  # [B, 13]\n",
    "predicted = torch.argmax(logits, dim=1)\n",
    "#predicted_answers = [idx_to_answer[idx.item()] for idx in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer_ems2[predicted])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
