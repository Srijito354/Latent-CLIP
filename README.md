# Modalities got latent: A novel approach to apply multi-head latent attention to multi-modal models.

Team IkAI presents to you a novel approach to solving multi-modal problems using what we call, *"Latent processing"*:
This approach was heavily inspired by the Multi-head Latent Attention paper as published by DeepSeek, back in 2024.
However, instead of applying latent attention technique only during the caching time, Latent processing compresses the Query, Key, and Value vectors at the very beginning before passing them to the transformer encoder blocks.
![image](https://github.com/user-attachments/assets/79327f67-0100-4f47-92ae-e5ef1eed3b2e)
